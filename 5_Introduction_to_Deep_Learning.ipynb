{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28d17300",
   "metadata": {},
   "source": [
    "<img src=\"graphics/header.png\" width=\"75%\"/>\n",
    "\n",
    "# **Introduction to Deep Learning**\n",
    "---\n",
    "\n",
    "<center><img src=\"graphics/artificial_neural_network.png\" width = \"30%\"/></center>\n",
    "\n",
    "In this module, we will introduce the principles and fundamental mechanics of **deep learning**, the subfield of AI associated with artificial neural networks (ANNs) and which is driving the current generative AI boom.\n",
    "\n",
    "\n",
    "After this module, learners will be able to:\n",
    "1. Describe the biological inspiration for artificial neural networks (ANN).\n",
    "2. Explain at a basic level how the Perceptron model (one of the simplest ANNs) processes input data.\n",
    "3. Identify the important role of activation functions in ANNs.\n",
    "3. Demonstrate your understanding of the Keras API by creating a simple neural network.\n",
    "\n",
    "### **üöÄ Let's get started.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8919db44-1f0e-4831-936e-0769ec03a103",
   "metadata": {},
   "source": [
    "# Biological Neurons\n",
    "The architecture of artificial neural networks was originally inspired by the structural anatomy of a biological neuron.  In a living brain, billions of neurons are connected together, forming a dense and complex network.\n",
    "\n",
    "<figure><img src=\"graphics/nn.png\" />\n",
    "    <figcaption style=\"text-align:center;font-weight:bold;\">\n",
    "        (a) Biological neurons and (b) their artificial counterparts.\n",
    "    </figcaption>\n",
    "</figure>\n",
    "\n",
    "### **Biological neurons...**\n",
    "* Use electrical and chemical signals to pass information between different regions of the brain.\n",
    "* Receive information through their dendrites and cell body.\n",
    "* Transmit eletrical signals down their axons which triggers the release of neurotransmitters through their axon terminals.\n",
    "\n",
    "In the context of ANNs, these processes are comparable to the flow of data, information, and error signals resulting from data samples being passed to the network's input layer.\n",
    "\n",
    "One key difference: the electrical signals sent down biologcial axons do **not** vary in magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5407a734-876c-4b4c-afed-bcc24796bbdc",
   "metadata": {},
   "source": [
    "# Perceptron (Artificial Neuron)\n",
    "\n",
    "A perceptron is a type of artificial neuron or computational unit used in artificial neural networks. A perceptron - also known as a *node* in deep learning - contains two functions, a net input function that sums up incoming inputs and an **activation** function. \n",
    "\n",
    "### **Perceptrons...**\n",
    "* Are one of the simplest ANNs.\n",
    "* Were inspired by biological neurons.\n",
    "* Receive information through the lines (weights) displayed to the left.\n",
    "* Apply an activation function to their received signal and output the result.\n",
    "\n",
    "The numerical signals that are output from ANNs **do** vary in magnitude!\n",
    "\n",
    "<center><img  src=\"graphics/perceptron.png\" alt='biological neuron'/></center>\n",
    "<center><a href=\"https://wiki.pathmind.com/neural-network\">Image source.</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d0db19",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "The choice of activation function is important as it can have a major impact on how well your model trains. Also, the non-linear transformation applied by activation functions allows networks to model a wide variety of non-linear problems.\n",
    "\n",
    "### **Activation functions..**\n",
    "* Are simple functions that transforms input values.\n",
    "* Are used in ANNs to map neuron inputs to neuron outputs (also fittingly referred to as neuron *activations*).\n",
    "\n",
    "<center><img width=\"500\" src=\"graphics/activation_functions.webp\"/></center>\n",
    "<center><a href=\"https://machine-learning.paperspace.com/wiki/activation-function\">Image source.</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38016ce3",
   "metadata": {},
   "source": [
    "> **‚úèÔ∏è Exercise:** Given a single neuron with input **x**, write a simple **linear activation** function using two lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecf3f332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code to replicate a simple linear activation function (technically requires just two lines of code).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e19cd29",
   "metadata": {},
   "source": [
    "> **‚úèÔ∏è Exercise:** Given a single neuron with input **x**, write a simple **ReLU activation** function using two lines of code. **Hint:** we can use the built-in Python function `max`. When you pass two numbers `a` and `b` into the max function as `max(a,b)`, the function will return the larger value. *Examples:* <code>max(-2, 3) = 3</code> or <code>max(1.5, 1.1) = 1.5</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d33784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code for a simple ReLU activation function in this code block.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f01e3b9",
   "metadata": {},
   "source": [
    "> **ü§î Question:** Is the ReLU a **linear** or **nonlinear** activation function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "000d42e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain why the ReLU activation function is either linear or nonlinear.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b20657-0b9e-4ff2-89c7-91bc26342d78",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks (ANNs)\n",
    "An artificial neural network (ANN) is comprised of artificial neurons connected together in layers. \n",
    "\n",
    "ANNs are...\n",
    "* A subset of machine learning that can be used to model a wide range of problems.\n",
    "* Composed of a collection of interconnected connected nodes that accept an input and produce an output.\n",
    "* A fundamental component of deep learning.\n",
    "\n",
    "<center><img src=\"graphics/neural_network_im.jpg\" width=\"55%\" alt=\"Neural Network\" style=\"background:white;\"/></center>\n",
    "\n",
    "<center><a href=\"https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks\">Image source</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f66ab33-169f-46c3-b901-d296cdf39b97",
   "metadata": {},
   "source": [
    "# Training a Neural Network\n",
    "\n",
    "Training a neural network is the process of teaching the model to perform a specific task or learn a particular pattern from a given set of training data. It involves adjusting the model's internal parameters or weights to minimize the difference between its predictions and the desired outputs.\n",
    "\n",
    "Assuming that training data has been prepared, the training process includes the following steps:\n",
    "\n",
    "1. **Forward Propagation:** The training data is fed into the model, and its input features are multiplied by the weights and passed through activation functions in each layer. \n",
    "2. **Loss (Error) Calculation:** The loss function calculates the network's total error, the difference between the model's predictions and the actual target values.\n",
    "3. **Backpropagation:** The gradients of the loss function with respect to the model's parameters are computed using the chain rule of calculus. \n",
    "4. **Parameter / Weight Update:**  The model's parameters are updated using optimization algorithms like stochastic gradient descent (SGD) or its variants.\n",
    "5. **Iterate:** Repeat the first four steps for each training run or epoch.  Each time we run through the entire training dataset, we say that we trained for one *epoch*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4a10fc-ee69-43df-a66b-c5aaf71a82ed",
   "metadata": {},
   "source": [
    "# Creating an Artificial Neural Network (Multi-Layer Perceptron)\n",
    "* An important part of an AI research project is evaluating different models to identify which works best for a given dataset and task.\n",
    "* Let's train a deep learning model on the dataset from our machine learning notebook.\n",
    "* This time, we'll be using a deep learning (DL) model that uses an artificial neural network called the multi-layer perceptron (MLP).\n",
    "* We will be using the user-friendly Python library `Keras` to build our MLP model.\n",
    "\n",
    "A full description of DL or the MLP is beyond the scope of this lesson. We refer interested learners to [Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT press](https://www.deeplearningbook.org).\n",
    "\n",
    "> **üéóÔ∏èKnowledge check:**\n",
    "> * Artificial neural networks were initially motivated by biological inspiration, but currently ANNs only loosely resemble the human brain.\n",
    "> * A DL model like the MLP is often defined in terms of **layers** (one *input layer*, one or more *hidden layers*, and one *output layer*). The more hidden layers that are added to a model, the \"deeper\" it gets.\n",
    "> * Each hidden layer is designed to transform the data from the layer before.\n",
    "> * The final layer learns to predict an outcome based on all of these transformations.\n",
    "\n",
    "<center><img src=\"graphics/ann.jpg\" alt=\"ANN\" width=\"400\" height=\"400\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d578aa",
   "metadata": {},
   "source": [
    "First, let's load our dataset and split it into training and testing sets. (You likely saw this in a previous module, so we'll skip the explanation).\n",
    "\n",
    "Just to change things up a bit, let's use **70% of the dataset to train** our model and **30% of the dataset to test** our model. (Last time we used *80%* for training and *20%* for testing.)\n",
    "\n",
    "We will accomplish this in the `train_test_split` function by changing the `test_size` parameter to be equal to `0.3`, representing **30%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dee8110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('https://www.dropbox.com/scl/fi/zzsy0dlbwn8vqk9vaqaar/data_processed.csv?rlkey=05u63ywmemb3ubu9kday4p23g&dl=1')\n",
    "X = df.iloc[:, 1:-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09495a56-0d85-4e70-b3e0-84e7584baae0",
   "metadata": {},
   "source": [
    "Let's create our first MLP model with the following specification:\n",
    "* An input layer with `47` units (one per input variable)\n",
    "* `1` hidden layer with `128` hidden units\n",
    "* An additional `1` hidden layer with `64` units\n",
    "* An output layer with `1` unit (for predicting 0 or 1 corresponding to our AKI outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef4d3e1a-68fe-4c37-9a6e-addcc00ed4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 00:32:25.575291: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2024-04-21 00:32:25.575330: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-04-21 00:32:25.575691: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "# Add the first hidden layer with 128 neurons (and implicitly create the input layer by specifying the \"input_dim\")\n",
    "model.add(layers.Dense(units=128, input_dim=47, activation='relu'))\n",
    "\n",
    "# Add the second hidden layer with 64 neurons\n",
    "model.add(layers.Dense(units=64, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model for a classification problem such as ours.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2054713-5ac3-4685-b617-a521ed0f02b7",
   "metadata": {},
   "source": [
    "### üïµÔ∏è A Deeper Dive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4827ad1-15b0-47f3-bb92-1eb20471d1c5",
   "metadata": {},
   "source": [
    "Each hidden layer of our neural network will be created using the **Dense** class from Keras. For each layer, we must define the number of hidden units (also known as neurons). There are several optional arguments we may also pass, which can be viewed in the [Keras documentation page](https://keras.io/api/layers/core_layers/dense/). We can add many layers to our deep learning model using the .add() function of the Sequential class. You can think of a Sequential container as a list of hidden layers.\n",
    "\n",
    "For the first layer of our neural network, we must tell Keras how many variables to expect in each input vector. From our previous data exploration, we know that each patient is defined by `47` different variables, so the input dimension to our network is `47`.\n",
    "\n",
    "One reason why deep learning models are so powerful is their ability to model complex variable interactions through nonlinear activation functions. We have several choices for activation function. In our example, we will use the commonly chosen Rectified Linear Unit activation (ReLU).\n",
    "\n",
    "Once we are satisfied with the hidden layers of our model, we need to add an output layer for generating class predictions. Our output layer will also be a Dense layer, but it will only have a single (1) unit. Instead of ReLU, we will use a sigmoid activation function, which is typically chosen for binary classification problems such as ours. Using a sigmoid activation on our output layer allows us to interpret the output as a prediction probability. In other words, the probability that a given input vector belongs to class 1.\n",
    "\n",
    "Now that we have defined the architecture of our neural network, we will use the .compile() function to build it. In our example we are defining a few arguments that are associated with the training of our model:\n",
    "* We are using a binary cross-entropy loss. This is an appropriate choise for binary classification.\n",
    "* We will be using the Adam optimizer, which is a popular version of stochastic gradient descent (SGD).\n",
    "* For this example, we are interested in our model's prediction accuracy, so we'll tell Keras to use the \"accuracy\" metric. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb74d1b6-52f0-4c1a-9227-496b5bd665f9",
   "metadata": {},
   "source": [
    "# Training our neural network\n",
    "\n",
    "Now it's time to train our prediction model! We will train (or, \"fit\") the model using our training dataset that we developed in a previous module.\n",
    "* We will use the one-line function `.fit()` to train our entire deep learning model.\n",
    "* We will specify some additional parameters to be used during the training process:\n",
    "    * We will tell Keras to train the model for `10` epochs.\n",
    "    * We will use a batch size of `64` samples. During each epoch, the model will pass in `64` samples at a time.\n",
    "    * We will use a random `30%` of the training dataset as our **validation set** (different from the test set) for computing metrics while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "768842f8-6a8d-42ae-8971-48495ca7e6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - 1s 30ms/step - loss: 0.6340 - accuracy: 0.6871 - val_loss: 0.5385 - val_accuracy: 0.7725\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.4980 - accuracy: 0.8262 - val_loss: 0.5331 - val_accuracy: 0.7820\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.4395 - accuracy: 0.8323 - val_loss: 0.5001 - val_accuracy: 0.7678\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.3921 - accuracy: 0.8487 - val_loss: 0.5007 - val_accuracy: 0.7536\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.3660 - accuracy: 0.8650 - val_loss: 0.4926 - val_accuracy: 0.7488\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.3430 - accuracy: 0.8671 - val_loss: 0.4804 - val_accuracy: 0.7678\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.3262 - accuracy: 0.8671 - val_loss: 0.4756 - val_accuracy: 0.7725\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.3092 - accuracy: 0.8712 - val_loss: 0.4795 - val_accuracy: 0.7725\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2927 - accuracy: 0.8834 - val_loss: 0.4787 - val_accuracy: 0.7773\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2774 - accuracy: 0.8793 - val_loss: 0.4807 - val_accuracy: 0.7867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f55081dc730>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split= 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e764a18-057e-4b51-b133-0b510f175e9e",
   "metadata": {},
   "source": [
    "### **Done!üéâ**\n",
    "Before we celebrate too much, let's check the performance of our trained model on the test set that we already set aside.\n",
    "\n",
    "**The model has never seen this particular data**, so our trained model's performance on the test set can provide us with an idea of how well the model might perform in the future (i.e., ***generalizability to unseen data***, one of the fundamental goals of machine learning).\n",
    "\n",
    "We will use our model's `.evaluate()` function to compute the loss, as well as any metrics that we defined when compiling our model.\n",
    "\n",
    "Since we told Keras to use `accuracy` when we compiled the model, we will also see the model's accuracy on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aae39541-c61c-4d6c-9eb7-ecc49487da2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 2ms/step - loss: 0.4119 - accuracy: 0.8333\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491e396c-5045-43c5-87e8-b269e597eaf9",
   "metadata": {},
   "source": [
    "> **‚úèÔ∏è Exercise:** Develop an MLP with `3` hidden layers (instead of 2), with the following number of neurons in each hidden layer:\n",
    "> * Hidden layer 1: `512` neurons\n",
    "> * Hidden layer 2: `256` neurons\n",
    "> * Hidden layer 3: `128` neurons\n",
    "\n",
    "> Create, compile, train, and evaluate this new MLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc54a17e-0cb9-4721-91e2-87ee993fc26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it! (feel free to re-use most of the code from before)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ef5be7-e23a-4ead-ab76-38b4e14aa083",
   "metadata": {},
   "source": [
    "> **‚úèÔ∏è Exercise:** Create an MLP with `8` hidden layers (instead of 3), with **any number** of neurons in each hidden layer (get creative!). Code, compile, train, and evaluate this new MLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5489bbbe-7e1d-48f3-8371-84f0ad2fbade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code your own personal multilayer perceptron (MLP) in this code block.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e299b1",
   "metadata": {},
   "source": [
    "### **üèÜ Bonus: TensorFlow Neural Network Playground**\n",
    "<center><img src=\"graphics/tensorflow_logo.png\" /></center>\n",
    "\n",
    "Interactively design, visualize, and analyze a custom deep learning network inside your own browser with the [TensorFlow Neural Network Playground](https://playground.tensorflow.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cd6d20-a1fb-4413-9b82-27481f3f7ee1",
   "metadata": {},
   "source": [
    "---\n",
    "> üîó Portions of this module are based on concepts from the University of Florida [Practicum AI](https://practicumai.org) course *Fundamentals of Deep Learning*.\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
